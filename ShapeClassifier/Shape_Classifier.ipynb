{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28031a1",
   "metadata": {},
   "source": [
    "# Shape Classifier: Circles, Squares, and Triangles\n",
    "\n",
    "A Jupyter Notebook to demonstrate the training and evaluation of a CNN model to classify synthetic images of geometric shapes.\n",
    "\n",
    "**Project Structure Note:** In adherence to the project requirement for `clean, modular, and well-organized code,` complex helper functions for data generation and visualization are encapsulated in the `ShapeClassifier/utils/` directory. This keeps the main notebook focused on the core machine learning workflow: configuration, data loading, model definition, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88344dde",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "This section handles all the initial setup for the project:\n",
    "*   Importing essential libraries.\n",
    "*   Defining constants, hyperparameters, and file paths.\n",
    "*   Setting up reproducibility seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17010b2",
   "metadata": {},
   "source": [
    "### 1.1. Path Configuration\n",
    "\n",
    "First, we set up the system path to ensure our utility scripts in the `ShapeClassifier/utils/` directory can be imported reliably. We also define the core paths for our project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a1ea52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\gilda\\OneDrive\\Documents\\My_Projects\\AI_Based_Projects\\shape-classifier-interview\\shape-classifier-pytorch' to system path.\n",
      "Project Root: c:\\Users\\gilda\\OneDrive\\Documents\\My_Projects\\AI_Based_Projects\\shape-classifier-interview\\shape-classifier-pytorch\n",
      "Dataset Directory: c:\\Users\\gilda\\OneDrive\\Documents\\My_Projects\\AI_Based_Projects\\shape-classifier-interview\\shape-classifier-pytorch\\..\\shape-classifier-artifacts\\shape-classifier-datasets\\ShapeClassifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# We assume this notebook is run from its location in the 'ShapeClassifier' directory.\n",
    "# The project root is one level up.\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"Added '{PROJECT_ROOT}' to system path.\")\n",
    "\n",
    "# Define core paths relative to the project root for robustness.\n",
    "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, '..', 'shape-classifier-artifacts')\n",
    "DATASET_DIR = os.path.join(ARTIFACTS_DIR, 'shape-classifier-datasets', 'ShapeClassifier')\n",
    "MODEL_CHECKPOINT_DIR = os.path.join(ARTIFACTS_DIR, 'shape-classifier-models', 'ShapeClassifier')\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Dataset Directory: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cc40b",
   "metadata": {},
   "source": [
    "### 1.2. Library Imports\n",
    "\n",
    "Next, we import all the necessary libraries for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0ff414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c377a5",
   "metadata": {},
   "source": [
    "### 1.3. Constants and Seeding\n",
    "\n",
    "We define key constants for our project and a function to set seeds for reproducibility. The seeding function is called in a separate cell to prevent it from being re-run unnecessarily when constants are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4522ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: CUDA\n"
     ]
    }
   ],
   "source": [
    "# --- Constants ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLASSES = ['circle', 'square', 'triangle']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "MODEL_CHECKPOINT_PATH = os.path.join(MODEL_CHECKPOINT_DIR, 'best_model_checkpoint.pth')\n",
    "\n",
    "\n",
    "# --- Seeding Function ---\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for random, numpy, and torch for reproducible results.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "print(f\"Device set to: {DEVICE.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370723aa",
   "metadata": {},
   "source": [
    "### 1.4. Initial Setup Execution\n",
    "\n",
    "Here we execute one-time setup commands: we set the global seed and ensure the directory for saving our model checkpoints exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a3130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Initial setup executed.\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for the entire notebook session.\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure the directory for saving model checkpoints exists.\n",
    "os.makedirs(MODEL_CHECKPOINT_DIR, exist_ok=True)\n",
    "print(\"Initial setup executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb0336",
   "metadata": {},
   "source": [
    "## 2. Data Generation & Loading\n",
    "\n",
    "This section covers preparing the data for the model. It includes:\n",
    "*   An optional step to regenerate the entire dataset from scratch.\n",
    "*   Defining a custom PyTorch `Dataset` class to load the images.\n",
    "*   Applying necessary transformations.\n",
    "*   Creating `DataLoader`s to feed data to the model in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19a0c3",
   "metadata": {},
   "source": [
    "### 2.1. Optional: Generate Dataset from Scratch\n",
    "\n",
    "This project is configured to use the pre-generated (or downloaded) dataset located in the `shape-classifier-artifacts` directory.\n",
    "\n",
    "However, if you wish to regenerate the entire dataset from scratch, you can set the `REGENERATE_DATA` flag in the cell below to `True`.\n",
    "\n",
    "**Warning:** This process will delete all existing data in the target directory and will take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9974de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping data generation. Using existing dataset.\n",
      "\n",
      "Dataset found at: c:\\Users\\gilda\\OneDrive\\Documents\\My_Projects\\AI_Based_Projects\\shape-classifier-interview\\shape-classifier-pytorch\\..\\shape-classifier-artifacts\\shape-classifier-datasets\\ShapeClassifier\n"
     ]
    }
   ],
   "source": [
    "from ShapeClassifier.utils.data_generator import generate_dataset\n",
    "\n",
    "# --- Configuration for Data Generation ---\n",
    "REGENERATE_DATA = False # Set to True to run the data generation process.\n",
    "\n",
    "# These parameters are used ONLY if REGENERATE_DATA is True.\n",
    "TOTAL_IMAGES_TO_GENERATE = 6000\n",
    "SPLIT_RATIOS = {'train': 0.7, 'validation': 0.15, 'test': 0.15}\n",
    "\n",
    "\n",
    "if REGENERATE_DATA:\n",
    "    # We call our imported function using the path constants defined in Section 1.\n",
    "    generate_dataset(\n",
    "        root_dir=DATASET_DIR,\n",
    "        total_images=TOTAL_IMAGES_TO_GENERATE,\n",
    "        # root_dir=os.path.join(DATASET_DIR, \"Delete\"),\n",
    "        # total_images=100,\n",
    "        splits=SPLIT_RATIOS,\n",
    "        shapes=CLASSES # Using the 'CLASSES' constant from Section 1\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping data generation. Using existing dataset.\")\n",
    "\n",
    "# Verify that the dataset directory exists.\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    print(\"\\nERROR: Dataset directory not found!\")\n",
    "    print(f\"Please run the project's download script or set REGENERATE_DATA to True.\")\n",
    "else:\n",
    "    print(f\"\\nDataset found at: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b9db0",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "Here, we define the architecture of our Convolutional Neural Network (CNN) and provide the justification for this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba7d2c",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "This section contains the logic for training the model, including:\n",
    "*   Defining the loss function and optimizer.\n",
    "*   Implementing the training and validation loops.\n",
    "*   Incorporating early stopping and model checkpointing to save the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d5bcd",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "After training, we evaluate the model's performance on the unseen test set. This includes:\n",
    "*   Loading the best model checkpoint.\n",
    "*   Calculating final test accuracy.\n",
    "*   Visualizing a confusion matrix and sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfd960",
   "metadata": {},
   "source": [
    "## 6. Analysis & Conclusion\n",
    "\n",
    "A final summary of the results, including:\n",
    "*   Plotting the training/validation curves.\n",
    "*   Answering the project questions on learnings and potential improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape-classifier-pytorch-cUSr_hTk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
